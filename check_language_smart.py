#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ™ºèƒ½è¯­è¨€åŒ¹é…æ£€æŸ¥

è¿™ä¸ªè„šæœ¬å°†æ™ºèƒ½æ£€æµ‹ï¼š
1. æ­£ç¡®è¯†åˆ«emojiè¡¨æƒ…ç¬¦å·
2. åŒºåˆ†çœŸæ­£çš„è¯­è¨€æ··åˆå’Œemojiè£…é¥°
3. æä¾›å‡†ç¡®çš„è¯­è¨€åŒ¹é…æŠ¥å‘Š
"""

import os
import glob
import re

def get_guide_pages():
    """è·å–æ‰€æœ‰æ”»ç•¥é¡µé¢åˆ—è¡¨"""
    root_guides = glob.glob('*.html')
    guide_pages = []
    
    for page in root_guides:
        if page.startswith('how-to-') or page.startswith('ice-cream-') or page in ['guides.html', 'index.html', 'online.html']:
            guide_pages.append(page)
    
    return guide_pages

def is_emoji(char):
    """æ£€æŸ¥å­—ç¬¦æ˜¯å¦ä¸ºemoji"""
    # Emoji UnicodeèŒƒå›´
    emoji_ranges = [
        (0x1F600, 0x1F64F),  # è¡¨æƒ…ç¬¦å·
        (0x1F300, 0x1F5FF),  # æ‚é¡¹ç¬¦å·å’Œè±¡å½¢æ–‡å­—
        (0x1F680, 0x1F6FF),  # äº¤é€šå’Œåœ°å›¾ç¬¦å·
        (0x1F1E0, 0x1F1FF),  # åŒºåŸŸæŒ‡ç¤ºç¬¦å·
        (0x2600, 0x26FF),    # æ‚é¡¹ç¬¦å·
        (0x2700, 0x27BF),    # è£…é¥°ç¬¦å·
        (0xFE00, 0xFE0F),    # å˜ä½“é€‰æ‹©å™¨
        (0x1F900, 0x1F9FF),  # è¡¥å……ç¬¦å·å’Œè±¡å½¢æ–‡å­—
        (0x1F018, 0x1F270),  # å°é—­å­—æ¯æ•°å­—è¡¥å……
        (0x238C, 0x2454),    # å‡ ä½•å½¢çŠ¶
        (0x20D0, 0x20FF),    # ç»„åˆç”¨åŠè§’ç¬¦å·
    ]
    
    code_point = ord(char)
    for start, end in emoji_ranges:
        if start <= code_point <= end:
            return True
    return False

def detect_language_smart(text):
    """æ™ºèƒ½æ£€æµ‹æ–‡æœ¬è¯­è¨€ï¼Œå¿½ç•¥emoji"""
    
    # ç§»é™¤emojiï¼Œåªåˆ†æå®é™…æ–‡æœ¬
    text_without_emoji = ''.join(char for char in text if not is_emoji(char))
    
    if not text_without_emoji.strip():
        return 'emoji_only'
    
    # æ£€æµ‹è¯­è¨€
    chinese_chars = re.findall(r'[\u4e00-\u9fff]', text_without_emoji)
    japanese_chars = re.findall(r'[\u3040-\u309f\u30a0-\u30ff]', text_without_emoji)
    korean_chars = re.findall(r'[\uac00-\ud7af]', text_without_emoji)
    arabic_chars = re.findall(r'[\u0600-\u06ff]', text_without_emoji)
    hindi_chars = re.findall(r'[\u0900-\u097f]', text_without_emoji)
    
    if chinese_chars:
        return 'zh-cn'
    elif japanese_chars:
        return 'ja'
    elif korean_chars:
        return 'ko'
    elif arabic_chars:
        return 'ar'
    elif hindi_chars:
        return 'hi'
    else:
        # æ£€æŸ¥æ˜¯å¦åŒ…å«éè‹±æ–‡å­—ç¬¦
        non_english = re.findall(r'[^\x00-\x7f]', text_without_emoji)
        if non_english:
            # å°è¯•è¯†åˆ«å…¶ä»–è¯­è¨€
            if any(char in 'Ã¡Ã©Ã­Ã³ÃºÃ±Ã¼' for char in non_english):
                return 'es'
            elif any(char in 'Ã Ã¢Ã¤Ã©Ã¨ÃªÃ«Ã¯Ã®Ã´Ã¶Ã¹Ã»Ã¼Ã¿Ã§' for char in non_english):
                return 'fr'
            elif any(char in 'Ã¤Ã¶Ã¼ÃŸ' for char in non_english):
                return 'de'
            elif any(char in 'Ã Ã¡Ã¢Ã£Ã§Ã©ÃªÃ­Ã³Ã´ÃµÃº' for char in non_english):
                return 'pt-br'
            elif any(char in 'Ğ°-ÑÑ‘' for char in non_english):
                return 'ru'
            elif any(char in 'Ã Ã¡áº¡áº£Ã£Äƒáº¯áº±áº³áºµáº·Ã¢áº¥áº§áº©áº«áº­Ã©Ã¨áº»áº½áº¹Ãªáº¿á»á»ƒá»…á»‡Ã­Ã¬á»‰Ä©á»‹Ã³Ã²á»Ãµá»Ã´á»‘á»“á»•á»—á»™Æ¡á»›á»á»Ÿá»¡á»£ÃºÃ¹á»§Å©á»¥Æ°á»©á»«á»­á»¯á»±Ã½á»³á»·á»¹á»µÄ‘' for char in non_english):
                return 'vi'
            elif any(char in 'Ã Ã¡áº¡áº£Ã£Äƒáº¯áº±áº³áºµáº·Ã¢áº¥áº§áº©áº«áº­Ã©Ã¨áº»áº½áº¹Ãªáº¿á»á»ƒá»…á»‡Ã­Ã¬á»‰Ä©á»‹Ã³Ã²á»Ãµá»Ã´á»‘á»“á»•á»—á»™Æ¡á»›á»á»Ÿá»¡á»£ÃºÃ¹á»§Å©á»¥Æ°á»©á»«á»­á»¯á»±Ã½á»³á»·á»¹á»µÄ‘' for char in non_english):
                return 'id'
            else:
                return 'unknown'
        else:
            return 'en'

def check_page_language_consistency_smart(guide_page):
    """æ™ºèƒ½æ£€æŸ¥å•ä¸ªé¡µé¢çš„è¯­è¨€ä¸€è‡´æ€§"""
    
    print(f"\nğŸ“„ æ™ºèƒ½æ£€æŸ¥é¡µé¢: {guide_page}")
    print("-" * 50)
    
    try:
        with open(guide_page, 'r', encoding='utf-8') as f:
            content = f.read()
        
        issues = []
        
        # 1. æ£€æŸ¥é¡µé¢å£°æ˜çš„è¯­è¨€
        html_lang_match = re.search(r'<html lang="([^"]+)"', content)
        declared_lang = html_lang_match.group(1) if html_lang_match else 'unknown'
        print(f"  ğŸŒ é¡µé¢å£°æ˜è¯­è¨€: {declared_lang}")
        
        # 2. æ£€æŸ¥æ ‡é¢˜è¯­è¨€
        title_match = re.search(r'<title>([^<]+)</title>', content)
        if title_match:
            title = title_match.group(1)
            title_lang = detect_language_smart(title)
            print(f"  ğŸ“ æ ‡é¢˜è¯­è¨€: {title_lang} - {title[:50]}...")
            
            if title_lang != declared_lang and title_lang not in ['unknown', 'emoji_only']:
                issues.append(f"æ ‡é¢˜è¯­è¨€ä¸åŒ¹é…: å£°æ˜{declared_lang}, å®é™…{title_lang}")
        
        # 3. æ£€æŸ¥æè¿°è¯­è¨€
        desc_match = re.search(r'<meta name="description" content="([^"]+)"', content)
        if desc_match:
            desc = desc_match.group(1)
            desc_lang = detect_language_smart(desc)
            print(f"  ğŸ“‹ æè¿°è¯­è¨€: {desc_lang} - {desc[:50]}...")
            
            if desc_lang != declared_lang and desc_lang not in ['unknown', 'emoji_only']:
                issues.append(f"æè¿°è¯­è¨€ä¸åŒ¹é…: å£°æ˜{declared_lang}, å®é™…{desc_lang}")
        
        # 4. æ£€æŸ¥ä¸»è¦å†…å®¹è¯­è¨€
        content_tags = re.findall(r'<(h[1-3]|p)[^>]*>([^<]+)</\1>', content)
        if content_tags:
            content_texts = [tag[1] for tag in content_tags]
            content_sample = ' '.join(content_texts[:5])
            content_lang = detect_language_smart(content_sample)
            print(f"  ğŸ“– å†…å®¹è¯­è¨€: {content_lang} - æ ·æœ¬: {content_sample[:100]}...")
            
            if content_lang != declared_lang and content_lang not in ['unknown', 'emoji_only']:
                issues.append(f"å†…å®¹è¯­è¨€ä¸åŒ¹é…: å£°æ˜{declared_lang}, å®é™…{content_lang}")
        
        # 5. æ£€æŸ¥å¯¼èˆªé“¾æ¥è¯­è¨€
        nav_links = re.findall(r'<a[^>]*>([^<]+)</a>', content)
        if nav_links:
            nav_texts = [link for link in nav_links if len(link.strip()) > 3]
            if nav_texts:
                nav_sample = ' '.join(nav_texts[:3])
                nav_lang = detect_language_smart(nav_sample)
                print(f"  ğŸ§­ å¯¼èˆªè¯­è¨€: {nav_lang} - æ ·æœ¬: {nav_sample[:50]}...")
                
                if nav_lang != declared_lang and nav_lang not in ['unknown', 'emoji_only']:
                    issues.append(f"å¯¼èˆªè¯­è¨€ä¸åŒ¹é…: å£°æ˜{declared_lang}, å®é™…{nav_lang}")
        
        # 6. æ™ºèƒ½æ£€æŸ¥æ··åˆè¯­è¨€é—®é¢˜ï¼ˆå¿½ç•¥emojiï¼‰
        # åªæ£€æŸ¥çœŸæ­£çš„æ–‡æœ¬æ··åˆï¼Œä¸åŒ…æ‹¬emojiè£…é¥°
        text_content = re.sub(r'<[^>]+>', '', content)  # ç§»é™¤HTMLæ ‡ç­¾
        text_content = re.sub(r'[^\w\s\u4e00-\u9fff\u3040-\u309f\u30a0-\u30ff\uac00-\ud7af\u0600-\u06ff\u0900-\u097f]', '', text_content)  # åªä¿ç•™æ–‡å­—å’Œç©ºæ ¼
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«ä¸­æ–‡å­—ç¬¦
        chinese_in_content = re.search(r'[\u4e00-\u9fff]', text_content)
        english_in_content = re.search(r'[a-zA-Z]', text_content)
        
        if chinese_in_content and english_in_content:
            # è¿›ä¸€æ­¥æ£€æŸ¥ï¼šå¦‚æœä¸­æ–‡å†…å®¹å¾ˆå°‘ï¼Œå¯èƒ½æ˜¯è¯¯åˆ¤
            chinese_chars = re.findall(r'[\u4e00-\u9fff]', text_content)
            english_chars = re.findall(r'[a-zA-Z]', text_content)
            
            if len(chinese_chars) < 5:  # å¦‚æœä¸­æ–‡å­—ç¬¦å¾ˆå°‘ï¼Œå¯èƒ½æ˜¯è¯¯åˆ¤
                print(f"  â„¹ï¸  æ£€æµ‹åˆ°å°‘é‡ä¸­æ–‡å­—ç¬¦({len(chinese_chars)}ä¸ª)ï¼Œå¯èƒ½æ˜¯è¯¯åˆ¤")
            else:
                issues.append("å‘ç°çœŸæ­£çš„ä¸­æ–‡+è‹±æ–‡æ··åˆå†…å®¹")
        
        # 7. æ£€æŸ¥ç‰¹å®šè¯­è¨€é—®é¢˜
        if declared_lang == 'en':
            # è‹±æ–‡é¡µé¢ä¸åº”è¯¥åŒ…å«å¤§é‡å…¶ä»–è¯­è¨€
            if chinese_in_content:
                chinese_chars = re.findall(r'[\u4e00-\u9fff]', text_content)
                if len(chinese_chars) > 10:  # å¦‚æœä¸­æ–‡å­—ç¬¦å¾ˆå¤šï¼Œæ‰è®¤ä¸ºæ˜¯é—®é¢˜
                    issues.append("è‹±æ–‡é¡µé¢åŒ…å«å¤§é‡ä¸­æ–‡å­—ç¬¦")
                else:
                    print(f"  â„¹ï¸  è‹±æ–‡é¡µé¢åŒ…å«å°‘é‡ä¸­æ–‡å­—ç¬¦({len(chinese_chars)}ä¸ª)ï¼Œå¯èƒ½æ˜¯è¯¯åˆ¤")
        
        # è¾“å‡ºæ£€æŸ¥ç»“æœ
        if issues:
            print(f"  âŒ å‘ç° {len(issues)} ä¸ªè¯­è¨€åŒ¹é…é—®é¢˜:")
            for issue in issues:
                print(f"    - {issue}")
            return False, issues
        else:
            print(f"  âœ… è¯­è¨€åŒ¹é…æ£€æŸ¥é€šè¿‡")
            return True, []
            
    except Exception as e:
        print(f"  âŒ æ£€æŸ¥å¤±è´¥: {str(e)}")
        return False, [f"æ£€æŸ¥å¤±è´¥: {str(e)}"]

def check_all_pages_language_consistency_smart():
    """æ™ºèƒ½æ£€æŸ¥æ‰€æœ‰é¡µé¢çš„è¯­è¨€ä¸€è‡´æ€§"""
    
    guide_pages = get_guide_pages()
    
    print("ğŸ” å¼€å§‹æ™ºèƒ½æ£€æŸ¥æ¯ä¸ªæ”»ç•¥é¡µé¢çš„è¯­è¨€åŒ¹é…é—®é¢˜...")
    print("=" * 80)
    
    total_pages = len(guide_pages)
    consistent_pages = 0
    inconsistent_pages = 0
    all_issues = []
    
    for guide_page in guide_pages:
        is_consistent, issues = check_page_language_consistency_smart(guide_page)
        
        if is_consistent:
            consistent_pages += 1
        else:
            inconsistent_pages += 1
            all_issues.extend(issues)
    
    # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
    print(f"\n" + "=" * 80)
    print("ğŸ“Š æ™ºèƒ½è¯­è¨€åŒ¹é…é—®é¢˜è¯¦ç»†æŠ¥å‘Š")
    print("=" * 80)
    
    print(f"\nğŸ“ˆ æ£€æŸ¥ç»“æœç»Ÿè®¡:")
    print(f"   - æ€»é¡µé¢æ•°: {total_pages}")
    print(f"   - è¯­è¨€ä¸€è‡´: {consistent_pages}")
    print(f"   - è¯­è¨€ä¸ä¸€è‡´: {inconsistent_pages}")
    print(f"   - ä¸€è‡´ç‡: {(consistent_pages/total_pages)*100:.1f}%")
    
    if all_issues:
        print(f"\nâŒ å‘ç°çš„é—®é¢˜ç±»å‹:")
        issue_counts = {}
        for issue in all_issues:
            issue_type = issue.split(':')[0] if ':' in issue else issue
            issue_counts[issue_type] = issue_counts.get(issue_type, 0) + 1
        
        for issue_type, count in sorted(issue_counts.items(), key=lambda x: x[1], reverse=True):
            print(f"   - {issue_type}: {count} æ¬¡")
        
        print(f"\nğŸ“‹ æ‰€æœ‰é—®é¢˜åˆ—è¡¨:")
        for i, issue in enumerate(all_issues, 1):
            print(f"   {i}. {issue}")
    
    print(f"\nğŸ’¡ æ”¹è¿›å»ºè®®:")
    if inconsistent_pages > 0:
        print(f"1. ğŸ”§ ä¿®å¤çœŸæ­£çš„è¯­è¨€ä¸åŒ¹é…é—®é¢˜")
        print(f"2. ğŸŒ ç¡®ä¿æ¯ä¸ªé¡µé¢çš„å£°æ˜è¯­è¨€ä¸å®é™…å†…å®¹è¯­è¨€ä¸€è‡´")
        print(f"3. ğŸ“ æ£€æŸ¥å¹¶ä¿®æ­£æ··åˆè¯­è¨€å†…å®¹")
        print(f"4. ğŸ” éªŒè¯å…ƒæ ‡ç­¾çš„è¯­è¨€ä¸€è‡´æ€§")
    else:
        print(f"âœ… æ‰€æœ‰é¡µé¢çš„è¯­è¨€åŒ¹é…æ£€æŸ¥éƒ½é€šè¿‡äº†ï¼")
    
    return consistent_pages, inconsistent_pages, all_issues

def main():
    """ä¸»å‡½æ•°"""
    
    print("ğŸ” å¼€å§‹æ™ºèƒ½æ£€æŸ¥æ¯ä¸ªæ”»ç•¥é¡µé¢çš„è¯­è¨€åŒ¹é…é—®é¢˜...")
    print("=" * 80)
    
    # æ£€æŸ¥æ‰€æœ‰é¡µé¢çš„è¯­è¨€ä¸€è‡´æ€§
    consistent_pages, inconsistent_pages, all_issues = check_all_pages_language_consistency_smart()
    
    print(f"\n" + "=" * 80)
    print("ğŸ‰ æ™ºèƒ½è¯­è¨€åŒ¹é…æ£€æŸ¥å®Œæˆï¼")
    print("=" * 80)
    
    if inconsistent_pages == 0:
        print("ğŸ‰ æ­å–œï¼æ‰€æœ‰é¡µé¢çš„è¯­è¨€åŒ¹é…æ£€æŸ¥éƒ½é€šè¿‡äº†ï¼")
        print("âœ… æ‚¨çš„ç½‘ç«™è¯­è¨€ä¸€è‡´æ€§éå¸¸å¥½ï¼")
    else:
        print(f"âš ï¸  å‘ç° {inconsistent_pages} ä¸ªé¡µé¢å­˜åœ¨è¯­è¨€åŒ¹é…é—®é¢˜")
        print(f"ğŸ”§ å»ºè®®ä¼˜å…ˆä¿®å¤è¿™äº›é—®é¢˜ä»¥æå‡ç”¨æˆ·ä½“éªŒ")

if __name__ == "__main__":
    main() 